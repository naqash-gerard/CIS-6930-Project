{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e627aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T20:58:31.087295Z",
     "iopub.status.busy": "2025-11-03T20:58:31.087033Z",
     "iopub.status.idle": "2025-11-03T20:58:45.834591Z",
     "shell.execute_reply": "2025-11-03T20:58:45.833782Z"
    },
    "papermill": {
     "duration": 14.752795,
     "end_time": "2025-11-03T20:58:45.836123",
     "exception": false,
     "start_time": "2025-11-03T20:58:31.083328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-03 20:58:32.511766: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762203512.699981      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762203512.752835      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow / Keras\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Audio Processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# System & Utilities\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7795ba54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T20:58:45.842700Z",
     "iopub.status.busy": "2025-11-03T20:58:45.841875Z",
     "iopub.status.idle": "2025-11-03T20:58:45.845506Z",
     "shell.execute_reply": "2025-11-03T20:58:45.844813Z"
    },
    "papermill": {
     "duration": 0.007624,
     "end_time": "2025-11-03T20:58:45.846587",
     "exception": false,
     "start_time": "2025-11-03T20:58:45.838963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path=\"/kaggle/input/audio-mfcc/MFCC/Train\"\n",
    "test_path=\"/kaggle/input/audio-mfcc/MFCC/Test\"\n",
    "val_path=\"/kaggle/input/audio-mfcc/MFCC/Validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff7da6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T20:58:45.851783Z",
     "iopub.status.busy": "2025-11-03T20:58:45.851532Z",
     "iopub.status.idle": "2025-11-03T20:58:59.384299Z",
     "shell.execute_reply": "2025-11-03T20:58:59.383538Z"
    },
    "papermill": {
     "duration": 13.537133,
     "end_time": "2025-11-03T20:58:59.385940",
     "exception": false,
     "start_time": "2025-11-03T20:58:45.848807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13956 images belonging to 2 classes.\n",
      "Found 1088 images belonging to 2 classes.\n",
      "Found 2826 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator( \n",
    "                                   rescale=1./255,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                  ) \n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "val_datagen = ImageDataGenerator(  rescale=1./255.0)\n",
    "training_set = train_datagen.flow_from_directory(train_path,\n",
    "                                                 target_size = (299,299),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "test_set = test_datagen.flow_from_directory(test_path,\n",
    "                                            target_size = (299,299),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary',\n",
    "                                            shuffle=False)\n",
    "val_set = val_datagen.flow_from_directory(val_path,\n",
    "                                            target_size = (299,299),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "# data input shape\n",
    "input_shape = (299, 299, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d473c9b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T20:58:59.392801Z",
     "iopub.status.busy": "2025-11-03T20:58:59.392565Z",
     "iopub.status.idle": "2025-11-03T23:03:09.003455Z",
     "shell.execute_reply": "2025-11-03T23:03:09.002676Z"
    },
    "papermill": {
     "duration": 7449.852587,
     "end_time": "2025-11-03T23:03:09.241827",
     "exception": false,
     "start_time": "2025-11-03T20:58:59.389240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762203540.564953      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1762203540.565602      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762203561.123727      81 service.cc:148] XLA service 0x7c60a04165b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1762203561.124472      81 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1762203561.124491      81 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1762203562.898745      81 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-11-03 20:59:28.269310: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng3{k11=0} for conv (f32[32,128,147,147]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,147,147]{3,2,1,0}, f32[128,128,1,1]{3,2,1,0}), window={size=1x1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
      "2025-11-03 20:59:28.477791: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.208584762s\n",
      "Trying algorithm eng3{k11=0} for conv (f32[32,128,147,147]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,147,147]{3,2,1,0}, f32[128,128,1,1]{3,2,1,0}), window={size=1x1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
      "E0000 00:00:1762203577.045413      81 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1762203577.199569      81 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1762203578.393786      81 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1762203578.534347      81 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1762203579.030660      81 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1762203579.180015      81 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "I0000 00:00:1762203584.160649      81 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 1s/step - accuracy: 0.5613 - loss: 0.6756 - val_accuracy: 0.7286 - val_loss: 0.5704 - learning_rate: 1.0000e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 822ms/step - accuracy: 0.7733 - loss: 0.4922 - val_accuracy: 0.7155 - val_loss: 0.5267 - learning_rate: 1.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 850ms/step - accuracy: 0.8181 - loss: 0.4074 - val_accuracy: 0.8015 - val_loss: 0.4200 - learning_rate: 1.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 856ms/step - accuracy: 0.8426 - loss: 0.3572 - val_accuracy: 0.8493 - val_loss: 0.3653 - learning_rate: 1.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 887ms/step - accuracy: 0.8600 - loss: 0.3276 - val_accuracy: 0.8464 - val_loss: 0.3476 - learning_rate: 1.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 835ms/step - accuracy: 0.8703 - loss: 0.3110 - val_accuracy: 0.8719 - val_loss: 0.3011 - learning_rate: 1.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 827ms/step - accuracy: 0.8764 - loss: 0.2911 - val_accuracy: 0.8800 - val_loss: 0.2869 - learning_rate: 1.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 835ms/step - accuracy: 0.8865 - loss: 0.2696 - val_accuracy: 0.8956 - val_loss: 0.2550 - learning_rate: 1.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 831ms/step - accuracy: 0.8916 - loss: 0.2598 - val_accuracy: 0.9038 - val_loss: 0.2434 - learning_rate: 1.0000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 832ms/step - accuracy: 0.8951 - loss: 0.2424 - val_accuracy: 0.9147 - val_loss: 0.2224 - learning_rate: 1.0000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 871ms/step - accuracy: 0.9112 - loss: 0.2291 - val_accuracy: 0.9186 - val_loss: 0.2052 - learning_rate: 1.0000e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 828ms/step - accuracy: 0.9065 - loss: 0.2263 - val_accuracy: 0.9151 - val_loss: 0.2148 - learning_rate: 1.0000e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 835ms/step - accuracy: 0.9127 - loss: 0.2181 - val_accuracy: 0.9289 - val_loss: 0.1940 - learning_rate: 1.0000e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 841ms/step - accuracy: 0.9140 - loss: 0.2061 - val_accuracy: 0.9306 - val_loss: 0.1852 - learning_rate: 1.0000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 826ms/step - accuracy: 0.9127 - loss: 0.2145 - val_accuracy: 0.9257 - val_loss: 0.1945 - learning_rate: 1.0000e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 828ms/step - accuracy: 0.9172 - loss: 0.1973 - val_accuracy: 0.9328 - val_loss: 0.1756 - learning_rate: 1.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 838ms/step - accuracy: 0.9223 - loss: 0.1872 - val_accuracy: 0.9409 - val_loss: 0.1578 - learning_rate: 1.0000e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 829ms/step - accuracy: 0.9239 - loss: 0.1868 - val_accuracy: 0.9437 - val_loss: 0.1584 - learning_rate: 1.0000e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 816ms/step - accuracy: 0.9306 - loss: 0.1784 - val_accuracy: 0.9391 - val_loss: 0.1589 - learning_rate: 1.0000e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m437/437\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 816ms/step - accuracy: 0.9334 - loss: 0.1748 - val_accuracy: 0.9466 - val_loss: 0.1487 - learning_rate: 5.0000e-06\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Input shape and weights path\n",
    "input_shape = (299, 299, 3)\n",
    "weights_path = '/kaggle/input/xceptionet-weights/xception_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "# Load Xception base model\n",
    "base_model = Xception(weights=weights_path, include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Unfreeze last 40 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[-40:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Reduced dropout and removed BatchNorm\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "combined_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-5)\n",
    "combined_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7)\n",
    "\n",
    "# Train model\n",
    "history = combined_model.fit(\n",
    "    training_set,\n",
    "    epochs=20,\n",
    "    validation_data=val_set,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d710d2e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:03:10.058156Z",
     "iopub.status.busy": "2025-11-03T23:03:10.057907Z",
     "iopub.status.idle": "2025-11-03T23:03:10.578272Z",
     "shell.execute_reply": "2025-11-03T23:03:10.577719Z"
    },
    "papermill": {
     "duration": 0.940198,
     "end_time": "2025-11-03T23:03:10.579787",
     "exception": false,
     "start_time": "2025-11-03T23:03:09.639589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "combined_model.save('/kaggle/working/Deepfake_Audio_Model(2).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99a82e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:03:11.301197Z",
     "iopub.status.busy": "2025-11-03T23:03:11.300931Z",
     "iopub.status.idle": "2025-11-03T23:03:30.868306Z",
     "shell.execute_reply": "2025-11-03T23:03:30.867743Z"
    },
    "papermill": {
     "duration": 19.927649,
     "end_time": "2025-11-03T23:03:30.869356",
     "exception": false,
     "start_time": "2025-11-03T23:03:10.941707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1088 images belonging to 2 classes.\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 381ms/step - accuracy: 0.7673 - loss: 0.5177\n",
      "Test Loss: 0.7697054743766785\n",
      "Test Accuracy: 0.6351103186607361\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "test_path=\"/kaggle/input/audio-mfcc/MFCC/Test\"\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory(test_path,\n",
    "                                            target_size = (299,299),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary',\n",
    "                                            shuffle=False)\n",
    "# Load the model\n",
    "model = load_model('/kaggle/working/Deepfake_Audio_Model(2).h5')\n",
    "\n",
    "loss, accuracy = model.evaluate(test_set)\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dff0020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:03:31.663996Z",
     "iopub.status.busy": "2025-11-03T23:03:31.663724Z",
     "iopub.status.idle": "2025-11-03T23:03:46.227122Z",
     "shell.execute_reply": "2025-11-03T23:03:46.226523Z"
    },
    "papermill": {
     "duration": 14.984073,
     "end_time": "2025-11-03T23:03:46.228237",
     "exception": false,
     "start_time": "2025-11-03T23:03:31.244164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 264ms/step\n",
      "F1 Score: 0.4287515762925599\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.55      0.85      0.67       544\n",
      "        real       0.68      0.31      0.43       544\n",
      "\n",
      "    accuracy                           0.58      1088\n",
      "   macro avg       0.62      0.58      0.55      1088\n",
      "weighted avg       0.62      0.58      0.55      1088\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Load the model\n",
    "model = load_model('/kaggle/input/latest-model/keras/default/1/Deepfake_Audio_Model(2).h5')\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_probs = model.predict(test_set)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()  # Threshold at 0.5 for binary classification\n",
    "\n",
    "# True labels\n",
    "y_true = test_set.classes\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Optional: Detailed report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=test_set.class_indices.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "323b1695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:03:46.961972Z",
     "iopub.status.busy": "2025-11-03T23:03:46.961697Z",
     "iopub.status.idle": "2025-11-03T23:04:04.929393Z",
     "shell.execute_reply": "2025-11-03T23:04:04.928814Z"
    },
    "papermill": {
     "duration": 18.331781,
     "end_time": "2025-11-03T23:04:04.930476",
     "exception": false,
     "start_time": "2025-11-03T23:03:46.598695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "The predicted class is: Fake\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Function to convert audio to MFCC and save as an image for prediction\n",
    "def audio_to_mfcc_image(audio_file_path, target_size=(299, 299)):\n",
    "    # Load the audio file using librosa\n",
    "    audio, sr = librosa.load(audio_file_path, sr=None)  # sr=None keeps the original sampling rate\n",
    "\n",
    "    # Extract MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr)\n",
    "    # Resize the MFCC to match the input size of the model (299x299)\n",
    "    mfcc_resized = librosa.util.fix_length(mfcc, size=target_size[0])\n",
    "    mfcc_resized = np.expand_dims(mfcc_resized, axis=-1)  # Add the channel dimension\n",
    "\n",
    "    # Convert MFCC to image format\n",
    "    mfcc_image = np.repeat(mfcc_resized, 3, axis=-1)  \n",
    "\n",
    "    # Resize the MFCC image to the target size\n",
    "    mfcc_image = np.resize(mfcc_image, (target_size[0], target_size[1], 3))\n",
    "\n",
    "    return mfcc_image\n",
    "\n",
    "# Load the trained model\n",
    "#model = combined_model\n",
    "model = load_model('/kaggle/input/latest-model/keras/default/1/Deepfake_Audio_Model(2).h5')\n",
    "\n",
    "# Define the path to the audio file\n",
    "audio_file_path = os.path.join('/kaggle/input/in-the-wild-audio-deepfake/release_in_the_wild/fake/10002.wav') \n",
    "\n",
    "# Convert audio file to MFCC image\n",
    "mfcc_image = audio_to_mfcc_image(audio_file_path)\n",
    "\n",
    "# Add an extra dimension to match the model input shape (batch dimension)\n",
    "mfcc_image = np.expand_dims(mfcc_image, axis=0)\n",
    "\n",
    "# Preprocess the MFCC image (same preprocessing used during training for Xception)\n",
    "mfcc_image = preprocess_input(mfcc_image)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict(mfcc_image)\n",
    "\n",
    "# Interpret the result\n",
    "predicted_class = 'Fake' if prediction[0][0] > 0.5 else 'Real'\n",
    "print(f'The predicted class is: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e738fe9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:04:05.720313Z",
     "iopub.status.busy": "2025-11-03T23:04:05.719844Z",
     "iopub.status.idle": "2025-11-03T23:04:10.996032Z",
     "shell.execute_reply": "2025-11-03T23:04:10.995280Z"
    },
    "papermill": {
     "duration": 5.698176,
     "end_time": "2025-11-03T23:04:10.997431",
     "exception": false,
     "start_time": "2025-11-03T23:04:05.299255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/392065778.py:13: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_file_path, sr=None)\n",
      "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\n",
      "üîç Prediction: Fake\n",
      "üìä Confidence: 0.90\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ==========================================================\n",
    "# Function to convert audio to MFCC image for prediction\n",
    "# ==========================================================\n",
    "def audio_to_mfcc_image(audio_file_path, target_size=(299, 299)):\n",
    "    try:\n",
    "        # Load the audio file using librosa\n",
    "        audio, sr = librosa.load(audio_file_path, sr=None)\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "\n",
    "        # Fix MFCC length to match model input size\n",
    "        mfcc_resized = librosa.util.fix_length(mfcc, size=target_size[0])\n",
    "\n",
    "        # Expand dimensions and convert to RGB-like array\n",
    "        mfcc_resized = np.expand_dims(mfcc_resized, axis=-1)\n",
    "        mfcc_image = np.repeat(mfcc_resized, 3, axis=-1)\n",
    "        mfcc_image = np.resize(mfcc_image, (target_size[0], target_size[1], 3))\n",
    "\n",
    "        return mfcc_image\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing audio file: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Load the trained model\n",
    "# ==========================================================\n",
    "try:\n",
    "    model_path = '/kaggle/input/latest-model/keras/default/1/Deepfake_Audio_Model(2).h5'\n",
    "    model = load_model(model_path)\n",
    "    print(\"‚úÖ Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    model = None\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Define your test audio file path here\n",
    "# ==========================================================\n",
    "# Example: Upload a file to your dataset and replace the path below\n",
    "audio_file_path = '/kaggle/input/testing-data-real/New Recording 3.wav'\n",
    "# or use your own uploaded file like:\n",
    "# audio_file_path = '/kaggle/input/my-audio-folder/sample.wav'\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Convert audio to MFCC image and predict\n",
    "# ==========================================================\n",
    "if model and os.path.exists(audio_file_path):\n",
    "    mfcc_image = audio_to_mfcc_image(audio_file_path)\n",
    "\n",
    "    if mfcc_image is not None:\n",
    "        # Add batch dimension\n",
    "        mfcc_image = np.expand_dims(mfcc_image, axis=0)\n",
    "\n",
    "        # Preprocess as required by Xception model\n",
    "        mfcc_image = preprocess_input(mfcc_image)\n",
    "\n",
    "        # Predict\n",
    "        try:\n",
    "            prediction = model.predict(mfcc_image)\n",
    "            predicted_class = 'Fake' if prediction[0][0] > 0.5 else 'Real'\n",
    "            confidence = prediction[0][0] if predicted_class == 'Fake' else 1 - prediction[0][0]\n",
    "\n",
    "            print(f\"\\nüîç Prediction: {predicted_class}\")\n",
    "            print(f\"üìä Confidence: {confidence:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during prediction: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Could not convert audio to MFCC image.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model not loaded or audio file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6af34e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T23:04:11.737214Z",
     "iopub.status.busy": "2025-11-03T23:04:11.736944Z",
     "iopub.status.idle": "2025-11-03T23:04:11.741954Z",
     "shell.execute_reply": "2025-11-03T23:04:11.741219Z"
    },
    "papermill": {
     "duration": 0.374872,
     "end_time": "2025-11-03T23:04:11.743086",
     "exception": false,
     "start_time": "2025-11-03T23:04:11.368214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fake': 6978, 'real': 6978}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(training_set.classes, return_counts=True)\n",
    "print(dict(zip(training_set.class_indices.keys(), counts)))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4555568,
     "sourceId": 8130934,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4836275,
     "sourceId": 8171572,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7175763,
     "sourceId": 11452583,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7453875,
     "sourceId": 11861990,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8643611,
     "sourceId": 13602303,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8643620,
     "sourceId": 13602319,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8643629,
     "sourceId": 13602337,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 350889,
     "modelInstanceId": 330035,
     "sourceId": 403694,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 353355,
     "modelInstanceId": 332419,
     "sourceId": 406829,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7548.649034,
   "end_time": "2025-11-03T23:04:15.721869",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-03T20:58:27.072835",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
